{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Iyly9cWxpO-I",
      "metadata": {
        "id": "Iyly9cWxpO-I"
      },
      "source": [
        "##  Image Classification with Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "94f910d4",
      "metadata": {
        "id": "94f910d4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Convert input image into a sequence of flattened 'patches'\n",
        "    which will be equivalent to tokens\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size: int = 32, patch_size:int = 4, in_channels: int = 3, embed_dim : int = 128):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.projection = nn.Linear(patch_size * patch_size * in_channels, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x : (B, C, H, W) -> (B, N, embed_dim)\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        P = self.patch_size\n",
        "        N = self.num_patches\n",
        "\n",
        "        # Reshape into patches (B, C, H/P, P, W/P, P)\n",
        "        x = x.unfold(2, P, P).unfold(3, P, P)\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous() # (B, N_h, N_w, C, P, P)\n",
        "        x = x.view(B, N, -1)\n",
        "        x = self.projection(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "gc9B7li3KstZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gc9B7li3KstZ",
        "outputId": "5562a3fb-658e-44d2-f0e7-0961e4313cbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PatchEmbedding test passed\n"
          ]
        }
      ],
      "source": [
        "# Sanity Test\n",
        "IMAGE_SIZE = 32\n",
        "PATCH_SIZE = 4\n",
        "EMBED_DIM = 8\n",
        "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
        "model = PatchEmbedding(IMAGE_SIZE, PATCH_SIZE, 3, EMBED_DIM)\n",
        "x = torch.randn(2, 3, IMAGE_SIZE, IMAGE_SIZE)  # Batch size of 2\n",
        "output = model(x)\n",
        "assert output.shape == (2, NUM_PATCHES, EMBED_DIM), f\"Unexpected shape: {output.shape}\"\n",
        "print(\"PatchEmbedding test passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "-jgyM3Nky7dL",
      "metadata": {
        "id": "-jgyM3Nky7dL"
      },
      "outputs": [],
      "source": [
        "class ViTTransformerEncoderCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Pre-Norm Transformer ecoder cell\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_dim, embed_dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x: (B, num_patches+1, embed_dim)\n",
        "        \"\"\"\n",
        "        # Pre-Norm Self-attention\n",
        "        residual = x\n",
        "        x = self.norm1(x)\n",
        "        x, _ = self.attn(x, x, x, need_weights=False)  # Self-attention\n",
        "        x = residual + x  # Add residual connection\n",
        "\n",
        "        # Pre-Norm feedforward\n",
        "        residual = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ffn(x)\n",
        "        x = residual + x  # add residual\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "idiPV4d4Osh1",
      "metadata": {
        "id": "idiPV4d4Osh1"
      },
      "outputs": [],
      "source": [
        "# Sanity test\n",
        "batch_size = 2\n",
        "encoder = ViTTransformerEncoderCell(EMBED_DIM, 8, 256)\n",
        "x = torch.randn(batch_size, NUM_PATCHES + 1, EMBED_DIM)\n",
        "\n",
        "output = encoder(x)\n",
        "assert output.shape == x.shape, f\"Expected shape {x.shape}, but got {output.shape}\"\n",
        "print(\"ViTTransformerEncoderCell test passed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "APBVmUk3KUUH",
      "metadata": {
        "id": "APBVmUk3KUUH"
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Vision Transformer (ViT)\n",
        "    \"\"\"\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, num_classes=10,\n",
        "                 embed_dim=128, num_heads=4, num_layers=4, ff_dim=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        # Learnable CLS Token & Positional encoding\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_encoding = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        # nn.init.trunc_normal_(self.pos_encoding, std=0.02)  # Stable init\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Transformer Encoder Stack (as ModuleList for flexibility)\n",
        "        self.encoder_stack = nn.ModuleList([ViTTransformerEncoderCell(embed_dim, num_heads, ff_dim, dropout)\n",
        "                                      for _ in range(num_layers)])\n",
        "\n",
        "        # Classification Head\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)  # (B, num_patches, embed_dim)\n",
        "\n",
        "        # Append CLS Token\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)  # (B, num_patches+1, embed_dim)\n",
        "\n",
        "        # Add Positional Encoding with Dropout\n",
        "        x = self.dropout(x + self.pos_encoding[:, :x.shape[1], :])\n",
        "\n",
        "        # Transformer Encoding\n",
        "        for layer in self.encoder_stack:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Use CLS token for classification\n",
        "        x = self.norm(x[:, 0])  # Extract CLS token output\n",
        "        y =\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "BziKlzcGRjEc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BziKlzcGRjEc",
        "outputId": "4517220f-83f8-4901-c125-053c806b6b2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity test passed so I don't have to\n"
          ]
        }
      ],
      "source": [
        "# sanity check just to be sure lol debugging is killing me please somebody release me from this suffering\n",
        "batch_size = 2\n",
        "IMAGE_SIZE = 32\n",
        "PATCH_SIZE = 4\n",
        "num_classes = 10\n",
        "embed_dim = 128\n",
        "num_heads = 4\n",
        "num_layers = 4\n",
        "ff_dim = 512\n",
        "\n",
        "# Initialize model\n",
        "model = VisionTransformer(IMAGE_SIZE, PATCH_SIZE, 3, num_classes,\n",
        "                            embed_dim, num_heads, num_layers, ff_dim)\n",
        "\n",
        "# Create a dummy batch of images (B, C, H, W)\n",
        "x = torch.randn(batch_size, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
        "\n",
        "# Forward pass\n",
        "output = model(x)\n",
        "\n",
        "# Check output shape\n",
        "assert output.shape == (batch_size, num_classes), f\"Expected shape {(batch_size, num_classes)}, but got {output.shape}\"\n",
        "print(\"Sanity test passed so I don't have to\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W3Yba9_pA36h",
      "metadata": {
        "id": "W3Yba9_pA36h"
      },
      "source": [
        "### Retrieve and load CIFAR10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "JNkyUTKHy7j8",
      "metadata": {
        "id": "JNkyUTKHy7j8"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
        "plt.rcParams['image.interpolation'] = 'nearest'\n",
        "plt.rcParams['image.cmap'] = 'gray'\n",
        "\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "def rel_error(x, y):\n",
        "    \"\"\" returns relative error \"\"\"\n",
        "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "BSum54onBDGN",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSum54onBDGN",
        "outputId": "f2a5c861-af6f-426f-ba86-9ccac06f118a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-03-17 22:03:34--  http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 170498071 (163M) [application/x-gzip]\n",
            "Saving to: ‘cifar-10-python.tar.gz’\n",
            "\n",
            "cifar-10-python.tar 100%[===================>] 162.60M  35.4MB/s    in 4.9s    \n",
            "\n",
            "2025-03-17 22:03:39 (33.4 MB/s) - ‘cifar-10-python.tar.gz’ saved [170498071/170498071]\n",
            "\n",
            "cifar-10-batches-py/\n",
            "cifar-10-batches-py/data_batch_4\n",
            "cifar-10-batches-py/readme.html\n",
            "cifar-10-batches-py/test_batch\n",
            "cifar-10-batches-py/data_batch_3\n",
            "cifar-10-batches-py/batches.meta\n",
            "cifar-10-batches-py/data_batch_2\n",
            "cifar-10-batches-py/data_batch_5\n",
            "cifar-10-batches-py/data_batch_1\n"
          ]
        }
      ],
      "source": [
        "# let's download the data\n",
        "# !mkdir ../datasets\n",
        "# !cd ../datasets\n",
        "\n",
        "# 1 -- Linux\n",
        "# 2 -- MacOS\n",
        "# 3 -- Command Prompt on Windows\n",
        "# 4 -- manually downloading the data\n",
        "choice = 1\n",
        "\n",
        "\n",
        "if choice == 1:\n",
        "    # should work well on Linux and in Powershell on Windows\n",
        "    !wget http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
        "elif choice == 2 or choice ==3:\n",
        "    # if wget is not available for you, try curl\n",
        "    # should work well on MacOS\n",
        "    !curl http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz --output cifar-10-python.tar.gz\n",
        "else:\n",
        "    print('Please manually download the data from http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and put it under the datasets folder.')\n",
        "!tar -xzvf cifar-10-python.tar.gz\n",
        "\n",
        "if choice==3:\n",
        "    !del cifar-10-python.tar.gz\n",
        "else:\n",
        "    !rm cifar-10-python.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "M-lcLdW9BUrO",
      "metadata": {
        "id": "M-lcLdW9BUrO"
      },
      "outputs": [],
      "source": [
        "# helpful functions to process and load the data\n",
        "from six.moves import cPickle as pickle\n",
        "import numpy as np\n",
        "import os\n",
        "from imageio import imread\n",
        "import platform\n",
        "\n",
        "def load_pickle(f):\n",
        "    version = platform.python_version_tuple()\n",
        "    if version[0] == '2':\n",
        "        return  pickle.load(f)\n",
        "    elif version[0] == '3':\n",
        "        return  pickle.load(f, encoding='latin1')\n",
        "    raise ValueError(\"invalid python version: {}\".format(version))\n",
        "\n",
        "def load_CIFAR_batch(filename):\n",
        "  \"\"\" load single batch of cifar \"\"\"\n",
        "  with open(filename, 'rb') as f:\n",
        "    datadict = load_pickle(f)\n",
        "    X = datadict['data']\n",
        "    Y = datadict['labels']\n",
        "    X = X.reshape(10000, 3, 32, 32).transpose(0,2,3,1).astype(\"float\")\n",
        "    Y = np.array(Y)\n",
        "    return X, Y\n",
        "\n",
        "def load_CIFAR10(ROOT):\n",
        "  \"\"\" load all of cifar \"\"\"\n",
        "  xs = []\n",
        "  ys = []\n",
        "  for b in range(1,6):\n",
        "    f = os.path.join(ROOT, 'data_batch_%d' % (b, ))\n",
        "    X, Y = load_CIFAR_batch(f)\n",
        "    xs.append(X)\n",
        "    ys.append(Y)\n",
        "  Xtr = np.concatenate(xs)\n",
        "  Ytr = np.concatenate(ys)\n",
        "  del X, Y\n",
        "  Xte, Yte = load_CIFAR_batch(os.path.join(ROOT, 'test_batch'))\n",
        "  return Xtr, Ytr, Xte, Yte\n",
        "\n",
        "\n",
        "def get_CIFAR10_data(cifar10_dir, num_training=49000, num_validation=1000, num_test=1000,\n",
        "                     subtract_mean=True):\n",
        "    \"\"\"\n",
        "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
        "    it for classifiers. These are the same steps as we used for the SVM, but\n",
        "    condensed to a single function.\n",
        "    \"\"\"\n",
        "    # Load the raw CIFAR-10 data\n",
        "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
        "\n",
        "    # Subsample the data\n",
        "    mask = list(range(num_training, num_training + num_validation))\n",
        "    X_val = X_train[mask]\n",
        "    y_val = y_train[mask]\n",
        "    mask = list(range(num_training))\n",
        "    X_train = X_train[mask]\n",
        "    y_train = y_train[mask]\n",
        "    mask = list(range(num_test))\n",
        "    X_test = X_test[mask]\n",
        "    y_test = y_test[mask]\n",
        "\n",
        "    # Normalize the data: subtract the mean image\n",
        "    if subtract_mean:\n",
        "      mean_image = np.mean(X_train, axis=0)\n",
        "      X_train -= mean_image\n",
        "      X_val -= mean_image\n",
        "      X_test -= mean_image\n",
        "\n",
        "    # Transpose so that channels come first\n",
        "    X_train = X_train.transpose(0, 3, 1, 2).copy()\n",
        "    X_val = X_val.transpose(0, 3, 1, 2).copy()\n",
        "    X_test = X_test.transpose(0, 3, 1, 2).copy()\n",
        "\n",
        "    # Package data into a dictionary\n",
        "    return {\n",
        "      'X_train': X_train, 'y_train': y_train,\n",
        "      'X_val': X_val, 'y_val': y_val,\n",
        "      'X_test': X_test, 'y_test': y_test,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "AL-6p4nxBXKm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AL-6p4nxBXKm",
        "outputId": "c6745afd-5ab3-475b-bdf3-7ca4db68567f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===\n",
            "For the split train\n",
            "shape: (49000, 3, 32, 32)\n",
            "data value range, min: -4.489820571085577, max: 0.8966644435551998\n",
            "\n",
            "===\n",
            "For the split val\n",
            "shape: (1000, 3, 32, 32)\n",
            "data value range, min: -4.489820571085577, max: 0.8966644435551998\n",
            "\n",
            "===\n",
            "For the split test\n",
            "shape: (1000, 3, 32, 32)\n",
            "data value range, min: -4.489820571085577, max: 0.8966644435551998\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load the (preprocessed) CIFAR10 data.\n",
        "cifar10_dir = './cifar-10-batches-py'\n",
        "\n",
        "data = get_CIFAR10_data(cifar10_dir, subtract_mean=True)\n",
        "\n",
        "pix_mean = (0.485, 0.456, 0.406)\n",
        "pix_std = (0.229, 0.224, 0.225)\n",
        "\n",
        "for c in range(3):\n",
        "    data['X_train'][:, c] = (data['X_train'][:, c] / 255 - pix_mean[c]) / pix_std[c]\n",
        "    data['X_val'][:, c] = (data['X_val'][:, c] / 255 - pix_mean[c]) / pix_std[c]\n",
        "    data['X_test'][:, c] = (data['X_test'][:, c] / 255 - pix_mean[c]) / pix_std[c]\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    print('===\\nFor the split {}'.format(split))\n",
        "    print('shape: {}'.format(data['X_{}'.format(split)].shape))\n",
        "    print('data value range, min: {}, max: {}\\n'.format(data['X_{}'.format(split)].min(), data['X_{}'.format(split)].max()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "zo6fx5gEGByS",
      "metadata": {
        "id": "zo6fx5gEGByS"
      },
      "outputs": [],
      "source": [
        "# no need to implement anything here\n",
        "def set_up_cifar10_data_loader(images, labels, batch_size, shuffle=True):\n",
        "    dataset = torch.utils.data.TensorDataset(torch.Tensor(images), torch.Tensor(labels))\n",
        "    data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=2)\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R-PgCy1uBk8G",
      "metadata": {
        "id": "R-PgCy1uBk8G"
      },
      "source": [
        "### Implement training and testing function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "DPYi654yB_67",
      "metadata": {
        "id": "DPYi654yB_67"
      },
      "outputs": [],
      "source": [
        "def train_val_model(model, train_data_loader, val_data_loader, loss_fn, optimizer, lr_scheduler, num_epochs, print_freq=50):\n",
        "    \"\"\"\n",
        "    Training and validating a the image classification model\n",
        "\n",
        "    Inputs:\n",
        "      - model: An image classification model\n",
        "      - data_loader: A data loader that will provide batched images and labels\n",
        "      - loss_fn: A loss function\n",
        "      - optimizer: optimizer lol\n",
        "      - lr_scheduler: Learning rate scheduler\n",
        "      - num_epochs: Number of epochs in total\n",
        "      - print_freq: Frequency to print training statistics\n",
        "\n",
        "    Output:\n",
        "      - model: Trained CNN model\n",
        "    \"\"\"\n",
        "\n",
        "    for epoch_i in range(num_epochs):\n",
        "        # set the model in the train mode so the batch norm layers will behave correctly\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0.0\n",
        "        running_total = 0.0\n",
        "        running_correct = 0.0\n",
        "        for i, batch_data in enumerate(train_data_loader):\n",
        "            # Every data instance is an image + label pair\n",
        "            images, labels = batch_data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward pass\n",
        "            logits = model(images)\n",
        "            predicted = torch.argmax(logits, dim=1)\n",
        "\n",
        "            # backward pass\n",
        "            target_labels = labels.type(torch.LongTensor)\n",
        "            target_labels = target_labels.cuda()\n",
        "            loss = loss_fn(logits, target_labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # optimize\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            running_total += labels.size(0)\n",
        "            running_correct += (predicted == labels).sum().item()\n",
        "            if i % print_freq == 0:    # print every certain number of mini-batches\n",
        "                running_loss = running_loss / print_freq\n",
        "                running_acc = running_correct / running_total * 100\n",
        "                last_lr = lr_scheduler.get_last_lr()[0]\n",
        "                print(f'[{epoch_i + 1}/{num_epochs}, {i + 1:5d}/{len(train_data_loader)}] loss: {running_loss:.3f} acc: {running_acc:.3f} lr: {last_lr:.5f}')\n",
        "                running_loss = 0.0\n",
        "                running_total = 0.0\n",
        "                running_correct = 0.0\n",
        "\n",
        "        # adjust the learning rate\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        val_acc = test_model(model, val_data_loader)\n",
        "        print(f'[{epoch_i + 1}/{num_epochs}] val acc: {val_acc:.3f}')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "mhbTbG4RGS3g",
      "metadata": {
        "id": "mhbTbG4RGS3g"
      },
      "outputs": [],
      "source": [
        "# Function to test an already trained model\n",
        "def test_model(model, data_loader):\n",
        "    \"\"\"\n",
        "    Compute accuracy of the model.\n",
        "\n",
        "    Inputs:\n",
        "      - model: An image classification model\n",
        "      - data_loader: A data loader that will provide batched images and labels\n",
        "    \"\"\"\n",
        "\n",
        "    # set the model in evaluation mode so the batch norm layers will behave correctly\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
        "    with torch.no_grad():\n",
        "        for batch_data in data_loader:\n",
        "            images, labels = batch_data\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            logits = model(images)\n",
        "            predicted = torch.argmax(logits, dim=1) #softmax preserves ranking anyway\n",
        "\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = 100 * correct // total\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vnJ1PNegDawH",
      "metadata": {
        "id": "vnJ1PNegDawH"
      },
      "source": [
        "### Train the ViT Image Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "x6Nq8eR0CABb",
      "metadata": {
        "id": "x6Nq8eR0CABb"
      },
      "outputs": [],
      "source": [
        "num_epochs = 3\n",
        "\n",
        "model = VisionTransformer(\n",
        "    img_size=32, patch_size=4, in_channels=3,\n",
        "    num_classes=10,\n",
        "    embed_dim=128, num_heads=4, num_layers=4, ff_dim=512,\n",
        "    dropout=0.1)\n",
        "\n",
        "batch_size = 64\n",
        "learning_rate = 0.0005\n",
        "momentum = 0.98\n",
        "lr_gamma = 0.1\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-2)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "AKrdYfRKJKyP",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKrdYfRKJKyP",
        "outputId": "e8270373-05e3-4fa9-d465-4c2c160ca0ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 766 batches in the training set.\n",
            "There are 16 batches in the validation set.\n",
            "There are 16 batches in the testing set.\n",
            "Number of parameters: 809.354K\n"
          ]
        }
      ],
      "source": [
        "# set up the data loaders\n",
        "# note the usage of the batch_size hyperparameter here\n",
        "train_loader = set_up_cifar10_data_loader(data['X_train'], data['y_train'], batch_size, shuffle=True)\n",
        "print(\"There are {} batches in the training set.\".format(len(train_loader)))\n",
        "\n",
        "val_loader = set_up_cifar10_data_loader(data['X_val'], data['y_val'], batch_size, shuffle=False)\n",
        "print(\"There are {} batches in the validation set.\".format(len(val_loader)))\n",
        "\n",
        "test_loader = set_up_cifar10_data_loader(data['X_test'], data['y_test'], batch_size, shuffle=False)\n",
        "print(\"There are {} batches in the testing set.\".format(len(test_loader)))\n",
        "\n",
        "num_params = sum(p.numel() for p in model.parameters())\n",
        "print('Number of parameters: {:.3f}K'.format(num_params / 1000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "YYwZSvOwJJx-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYwZSvOwJJx-",
        "outputId": "96cef5bb-c3f7-4c7c-bceb-a1700805d1e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/3,     1/766] loss: 0.048 acc: 9.375 lr: 0.00050\n",
            "[1/3,    51/766] loss: 2.220 acc: 16.562 lr: 0.00050\n",
            "[1/3,   101/766] loss: 2.076 acc: 21.250 lr: 0.00050\n",
            "[1/3,   151/766] loss: 1.941 acc: 24.844 lr: 0.00050\n",
            "[1/3,   201/766] loss: 1.863 acc: 28.844 lr: 0.00050\n",
            "[1/3,   251/766] loss: 1.857 acc: 29.844 lr: 0.00050\n",
            "[1/3,   301/766] loss: 1.784 acc: 33.156 lr: 0.00050\n",
            "[1/3,   351/766] loss: 1.763 acc: 33.719 lr: 0.00050\n",
            "[1/3,   401/766] loss: 1.747 acc: 35.312 lr: 0.00050\n",
            "[1/3,   451/766] loss: 1.696 acc: 36.969 lr: 0.00050\n",
            "[1/3,   501/766] loss: 1.695 acc: 36.406 lr: 0.00050\n",
            "[1/3,   551/766] loss: 1.677 acc: 37.250 lr: 0.00050\n",
            "[1/3,   601/766] loss: 1.622 acc: 40.625 lr: 0.00050\n",
            "[1/3,   651/766] loss: 1.601 acc: 40.125 lr: 0.00050\n",
            "[1/3,   701/766] loss: 1.579 acc: 41.938 lr: 0.00050\n",
            "[1/3,   751/766] loss: 1.540 acc: 44.469 lr: 0.00050\n",
            "[1/3] val acc: 41.000\n",
            "[2/3,     1/766] loss: 0.031 acc: 39.062 lr: 0.00049\n",
            "[2/3,    51/766] loss: 1.563 acc: 42.844 lr: 0.00049\n",
            "[2/3,   101/766] loss: 1.512 acc: 44.625 lr: 0.00049\n",
            "[2/3,   151/766] loss: 1.489 acc: 45.969 lr: 0.00049\n",
            "[2/3,   201/766] loss: 1.450 acc: 46.000 lr: 0.00049\n",
            "[2/3,   251/766] loss: 1.408 acc: 49.469 lr: 0.00049\n",
            "[2/3,   301/766] loss: 1.473 acc: 45.812 lr: 0.00049\n",
            "[2/3,   351/766] loss: 1.413 acc: 47.906 lr: 0.00049\n",
            "[2/3,   401/766] loss: 1.419 acc: 47.938 lr: 0.00049\n",
            "[2/3,   451/766] loss: 1.398 acc: 48.344 lr: 0.00049\n",
            "[2/3,   501/766] loss: 1.397 acc: 48.062 lr: 0.00049\n",
            "[2/3,   551/766] loss: 1.392 acc: 49.719 lr: 0.00049\n",
            "[2/3,   601/766] loss: 1.373 acc: 49.625 lr: 0.00049\n",
            "[2/3,   651/766] loss: 1.343 acc: 51.188 lr: 0.00049\n",
            "[2/3,   701/766] loss: 1.341 acc: 51.688 lr: 0.00049\n",
            "[2/3,   751/766] loss: 1.353 acc: 50.031 lr: 0.00049\n",
            "[2/3] val acc: 52.000\n",
            "[3/3,     1/766] loss: 0.026 acc: 56.250 lr: 0.00045\n",
            "[3/3,    51/766] loss: 1.302 acc: 52.500 lr: 0.00045\n",
            "[3/3,   101/766] loss: 1.289 acc: 52.406 lr: 0.00045\n",
            "[3/3,   151/766] loss: 1.312 acc: 51.625 lr: 0.00045\n",
            "[3/3,   201/766] loss: 1.268 acc: 53.188 lr: 0.00045\n",
            "[3/3,   251/766] loss: 1.280 acc: 52.750 lr: 0.00045\n",
            "[3/3,   301/766] loss: 1.280 acc: 53.938 lr: 0.00045\n",
            "[3/3,   351/766] loss: 1.274 acc: 53.750 lr: 0.00045\n",
            "[3/3,   401/766] loss: 1.254 acc: 53.719 lr: 0.00045\n",
            "[3/3,   451/766] loss: 1.289 acc: 53.250 lr: 0.00045\n",
            "[3/3,   501/766] loss: 1.198 acc: 55.750 lr: 0.00045\n",
            "[3/3,   551/766] loss: 1.271 acc: 53.125 lr: 0.00045\n",
            "[3/3,   601/766] loss: 1.248 acc: 54.656 lr: 0.00045\n",
            "[3/3,   651/766] loss: 1.242 acc: 55.906 lr: 0.00045\n",
            "[3/3,   701/766] loss: 1.226 acc: 55.812 lr: 0.00045\n",
            "[3/3,   751/766] loss: 1.221 acc: 55.500 lr: 0.00045\n",
            "[3/3] val acc: 56.000\n",
            "testing accuracy: 58.000\n"
          ]
        }
      ],
      "source": [
        "model = model.cuda()\n",
        "model = train_val_model(model, train_loader, val_loader, loss_fn, optimizer, scheduler, num_epochs)\n",
        "test_acc = test_model(model, test_loader)\n",
        "print(f\"testing accuracy: {test_acc:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
